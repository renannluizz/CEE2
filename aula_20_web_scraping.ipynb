{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renannluizz/CEE2/blob/main/aula_20_web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Raspagem de Dados Web\n",
        "\n",
        "Raspagem de dados (***Web Scraping***) é o processo de automatização para  extrair informações de sites da web. Isso envolve acessar as páginas, identificar elementos específicos no código HTML (como textos, imagens ou links) e converter esses dados em um formato estruturado, como tabelas ou arquivos CSV, para possibilitar sua análise ou utilização em aplicações."
      ],
      "metadata": {
        "id": "2zwpPaoggTMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principais bibliotecas\n",
        "\n",
        "Para realizar a raspagem, utilizaremos as seguintes bibliotecas:\n",
        "\n",
        "- `requests`: Para fazer requisições HTTP e obter o HTML das páginas.\n",
        "\n",
        "- `BeautifulSoup`: Para analisar e navegar pelo HTML obtido.\n",
        "\n",
        "Instale essas bibliotecas usando o seguinte comando:\n",
        "\n",
        "```\n",
        "pip install requests\n",
        "pip install beautifulsoup4\n",
        "```\n",
        "\n",
        "Uma vez instalado, você pode importar essas bibliotecas com os seguintes comandos:\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "08NG4vm1h72N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requisição\n",
        "\n",
        "  Para faze a requisição, vamos utilizar a seguinte função:\n",
        "\n",
        "  ```python\n",
        "  requests.get(url)\n",
        "  ```\n",
        "\n",
        "  Quando você chama essa função, o Python envia uma solicitação HTTP para o servidor associado à URL fornecida. O servidor responde com um status code (indicando sucesso, erro, etc.) e, normalmente, com o conteúdo solicitado, como HTML ou JSON.\n",
        "\n",
        "***Argumentos:***\n",
        "\n",
        "  - `url` (obrigatório): A URL para onde será enviada a solicitação GET.\n",
        "\n",
        "  - `params` (opcional): Um dicionário de parâmetros de consulta para ser incluído na URL.\n",
        "\n",
        "  - `headers` (opcional): Cabeçalhos HTTP personalizados, como User-Agent.\n",
        "\n",
        "  - `timeout` (opcional): Tempo limite para a resposta do servidor, em segundos.\n",
        "\n",
        "***Retorno:***\n",
        "\n",
        "A função retorna um objeto da classe `Response`, que contém informações sobre a resposta do servidor, incluindo:\n",
        "\n",
        "  - `response.status_code`: Código de status HTTP (200 para sucesso, 404 para \"não encontrado\", etc.).\n",
        "  - `response.text`: Conteúdo da resposta como texto (usado para HTML).\n",
        "  - `response.json()`: Converte o conteúdo da resposta para um objeto JSON, se aplicável.\n",
        "  - `response.content`: Conteúdo bruto da resposta, em bytes.\n",
        "\n",
        "  Obs: Se ocorrer um erro, ela gera uma exceção automaticamente.\n",
        "\n",
        "***Exemplo:***\n",
        "\n",
        "A célula abaixo apresenta um exemplo de como o uso dessa função pode ser implementada."
      ],
      "metadata": {
        "id": "eDOlbK63rrnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfWNa11IgBDq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Função para obter o conteúdo HTML de uma URL\n",
        "def obter_html(url):\n",
        "    try:\n",
        "        resposta = requests.get(url)\n",
        "        resposta.raise_for_status()  # Lança uma exceção se o código de status HTTP indicar erro (ex: 404, 500).\n",
        "        return resposta.text  # Retorna apenas o HTML\n",
        "    except requests.RequestException as e: # Caso ocorra algum erro (como conexão recusada, timeout, erro de HTTP, etc.), imprime uma mensagem e retorna None.\n",
        "        print(f\"Erro ao acessar a URL: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos então analisar o comportamento para algumas páginas comuns. Nesses exemplos, vamos apenas apresentar os primeiros 200 caracteres de cada página."
      ],
      "metadata": {
        "id": "iV1fQGNHwYor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = obter_html(\"https://www.google.com\")\n",
        "print(resposta[:200])"
      ],
      "metadata": {
        "id": "7_jxha7kwfQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = obter_html(\"https://g1.globo.com/\")\n",
        "print(resposta[:200])"
      ],
      "metadata": {
        "id": "4V6LTYH6wx_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desta forma, podemos ter acesso ao código html da página, mas isso não parece diretamente muito útil. No entanto, a biblioteca `BeautifulSoup` pode nos ajudar organizando as informações contidas dentro de uma página."
      ],
      "metadata": {
        "id": "XEGvAZ3-xmjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estruturação\n",
        "\n",
        "A estruturação da saída gerada pela função `BeautifulSoup` organiza o conteúdo HTML em uma árvore de elementos DOM (*Document Object Model*). Isso permite navegar pelo conteúdo como se fosse um objeto hierárquico, onde cada nó representa um elemento HTML (tags, atributos, texto, etc.).\n",
        "\n",
        "Vamos olhar um exemplo fictício de HTML bruto (suponha que tenha sido obtido por request):\n",
        "\n",
        "```html\n",
        "<html>\n",
        "  <head>\n",
        "    <title>Exemplo de Página</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1>Bem-vindo ao Tutorial</h1>\n",
        "    <p class=\"intro\">Este é um exemplo simples de raspagem.</p>\n",
        "    <a href=\"https://exemplo.com\">Leia mais</a>\n",
        "  </body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "Após rodar a requisição dessa página, vamos receber (via função `.text`) algo do tipo:\n",
        "```python\n",
        "\"<html><head><title>Exemplo de Página</title></head><body><h1>Bem-vindo ao Tutorial</h1><p class=\\\"intro\\\">Este é um exemplo simples de raspagem.</p><a href=\\\"https://exemplo.com\\\">Leia mais</a></body></html>\"\n",
        "```\n",
        "\n",
        "A função `BeautifulSoup` então é chamada passando a página como um argumento:\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html_content)\n",
        "```\n",
        "\n",
        "Desta forma, quando o conteúdo é analisado com `BeautifulSoup`, ele cria uma árvore hierárquica (`soup` no exemplo), onde:\n",
        "\n",
        "- O elemento `<html>` é a raiz.\n",
        "- Filhos são os elementos contidos diretamente dentro de outro elemento (como `<head>` e `<body>` no `<html>`).\n",
        "- Textos, atributos e subelementos são acessíveis como propriedades dos nós.\n",
        "\n",
        "\n",
        "***Argumentos:***\n",
        "\n",
        "- `html_content` (obrigatório): O código HTML ou XML bruto que será analisado. Normalmente, é obtido com bibliotecas como requests.\n",
        "\n",
        "- `parser` (opcional): Especifica o analisador (parser) a ser usado para processar o HTML. Alguns exemplos:\n",
        "  - 'html.parser': Parser embutido no Python. Funciona bem para a maioria dos casos.\n",
        "  - 'xml': Para processar documentos XML.\n",
        "\n",
        "***Retorno:***\n",
        "\n",
        "  A função retorna um objeto da classe `BeautifulSoup` que representa o HTML como uma árvore de elementos, onde cada nó é um objeto que pode ser manipulado, incluindo:\n",
        "\n",
        "  - `prettify()`: Apresenta o código em um formato estruturado;\n",
        "\n",
        "  - `find(tag, attributes)`: Encontra a primeira ocorrência de uma tag.  \n",
        "\n",
        "  - `find_all(tag, attributes)`: Encontra todas as ocorrências de uma tag.\n",
        "\n",
        "  - `get_text(strip=True)`: Extrai o texto de dentro de uma tag, removendo espaços extras.\n",
        "\n",
        "\n",
        "***Navegação:***\n",
        "\n",
        "1. Navegação Direta: Você pode acessar diretamente elementos específicos, como filhos imediatos:\n",
        "\n",
        "  ```python\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "  # Acessar a tag <html>\n",
        "  html = soup.html\n",
        "\n",
        "  # Acessar a tag <body> (filho direto de <html>)\n",
        "  body = html.body\n",
        "\n",
        "  # Acessar o título dentro da tag <head>\n",
        "  titulo = soup.head.title.text  # Saída: \"Exemplo de Página\"\n",
        "  ```\n",
        "\n",
        "2. Localização por Tags:\n",
        "Use `find()` e `find_all()` para buscar elementos específicos:\n",
        "\n",
        "  ```python\n",
        "  # Encontra o primeiro parágrafo <p>\n",
        "  paragrafo = soup.find('p')\n",
        "  print(paragrafo.text)  # Saída: \"Este é um exemplo simples de raspagem.\"\n",
        "\n",
        "  # Encontra todos os links <a>\n",
        "  links = soup.find_all('a')\n",
        "  for link in links:\n",
        "      print(link['href'])  # Saída: \"https://exemplo.com\"\n",
        "\n",
        "  ```\n",
        "\n",
        "3. Navegação por Atributos:\n",
        "Você pode buscar elementos que possuem atributos específicos, como classes ou IDs:\n",
        "\n",
        "  ```python\n",
        "  # Encontrar um elemento com uma classe específica\n",
        "  paragrafo_intro = soup.find('p', class_='intro')\n",
        "  print(paragrafo_intro.text)  # Saída: \"Este é um exemplo simples de raspagem.\"\n",
        "  ```\n",
        "\n",
        "4. Navegação pelo DOM:\n",
        "Use propriedades como parent, children, next_sibling, etc., para navegar pela estrutura:\n",
        "\n",
        "  ```python\n",
        "  # Navegar para o pai de um elemento\n",
        "  link = soup.find('a')\n",
        "  pai_do_link = link.parent\n",
        "  print(pai_do_link.name)  # Saída: \"body\"\n",
        "\n",
        "  # Navegar entre irmãos\n",
        "  paragrafo = soup.find('p')\n",
        "  proximo_elemento = paragrafo.next_sibling\n",
        "  print(proximo_elemento.name)  # Saída: \"a\" (o próximo irmão é o link <a>)\n",
        "\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "giXkfiqByWhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo: página Clima Tempo\n",
        "\n",
        "1. Vamos começar abrindo a página de interesse.\n",
        "  > Exemplo: abra https://www.climatempo.com.br/ e procure por Brasília-DF.\n",
        "\n",
        "2. Vamos então baixar a página e visualizar em um formato estruturado.\n",
        "  > Exemplo: foi utilizado https://www.climatempo.com.br/previsao-do-tempo/cidade/61/brasilia-df\n"
      ],
      "metadata": {
        "id": "-a6ijdDZ1KyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.climatempo.com.br/previsao-do-tempo/cidade/61/brasilia-df\"\n",
        "\n",
        "# Dicionário definindo tipo de exibição como navegador\n",
        "## você consegue essa informação indo na página e clicando em especionar página\n",
        "## e depois no campo Network\n",
        "## Pode ser necessario recarregar a pagina (F5)\n",
        "## * clique em qualquer item a esquerda, vá em header e busque pelas informacoes.\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\"\n",
        "}\n",
        "\n",
        "# Enviar a requisição\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "print( soup.prettify()[:1000] ) # apresenta o código em um formato estruturado"
      ],
      "metadata": {
        "id": "I23f1TayuS1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Código do título da página\n",
        "print( soup.head.title )      # saída: <title>Previsão do tempo e clima para hoje em Brasília - DF: Clique já!</title>\n",
        "\n",
        "## Texto do título da página\n",
        "print( soup.head.title.text ) # saída: Previsão do tempo e clima para hoje em Brasília - DF: Clique já!"
      ],
      "metadata": {
        "id": "lhfhq1PU_zWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Vamos agora inspecionar a página. Clique com o botão direito e vá em \"Inspecionar\". No menu que abre, selecione o ícone que corresponde a \"Select an element in the page to inspect it - Ctrl+Shit+C\"."
      ],
      "metadata": {
        "id": "11dC_imE_S-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Posicione então o mouse na informação desejada e clique.\n",
        "  * Note que você será direcionado para o trecho do código HTML que contém a informação que você clicou.\n",
        "  \n",
        "  * Desta forma, você pode utilizar as informações de tag, classe e outros para caputar a informação de interesse para o seu código.\n",
        "  \n",
        "  Veja os exemplos abaixo."
      ],
      "metadata": {
        "id": "wShxsDLgAzLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Temperatura mínima\n",
        "print( soup.find(\"span\", id = \"min-temp-1\").text )"
      ],
      "metadata": {
        "id": "XvrOBRAbEqY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Temperatura máxima\n",
        "print( soup.find(\"span\", id = \"max-temp-1\").text )"
      ],
      "metadata": {
        "id": "N_YzPjA_a1wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  Previsão de chuva\n",
        "lista = soup.find_all(\"span\", class_=\"_margin-l-5\")\n",
        "print(lista) ## note que temos uma lista de retornos\n",
        "\n",
        "## separando e limpando a informação de interesse\n",
        "texto_interesse = lista[1].text\n",
        "import re\n",
        "texto_interesse = re.sub(r\"\\s+\", \" \", texto_interesse) ## \"s+, REGEX para sequencia de espaços, tabulações, quebras de linha\"\n",
        "\n",
        "print(\"\\n\\nPrevisão de chuva:\", texto_interesse )"
      ],
      "metadata": {
        "id": "9M9Aw8iRcOYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo: manchetes do G1\n",
        "\n",
        "- Vamos começar inspecionando a página https://g1.globo.com/.\n",
        "\n",
        "- Note que as manchetes são apresentadas utilizando a tag `\"a\"` dentro da classe `\"feed-post-link\"`.\n",
        "\n",
        "- Desta forma, podemos utilizar o código abaixo para listar as manchetes:"
      ],
      "metadata": {
        "id": "r12FUnyMcIyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Manchetes do G1:\")\n",
        "\n",
        "url = \"https://g1.globo.com/\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encontrar manchetes\n",
        "    manchetes = soup.find_all(\"a\", class_=\"feed-post-link\")\n",
        "\n",
        "    for i, manchete in enumerate(manchetes, start=1):  # Mostrar as 5 primeiras\n",
        "        titulo = manchete.text\n",
        "        link = manchete['href']\n",
        "        print(f\"{i}. {titulo} - {link}\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Erro na requisição: {e}\")"
      ],
      "metadata": {
        "id": "9CA76N3Dlwxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo: frases do Pensador\n",
        "\n",
        "Ao inspecionar a página https://www.pensador.com/frases podemos notar que as frases estão distribuídas em tags `\"p\"` com classes `'frase'`.\n",
        "\n",
        "O código abaixo lista as 10 primeiras frases encontradas."
      ],
      "metadata": {
        "id": "2SD8rq7pgono"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Frases do Pensador:\")\n",
        "\n",
        "url = \"https://www.pensador.com/frases/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encontrar frases\n",
        "    frases = soup.find_all(\"p\", class_=\"frase\")\n",
        "    for i, frase in enumerate(frases[:10], start=1):  # Mostrar as 5 primeiras\n",
        "        print(f\"{i}. {frase.text.strip()}\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Erro na requisição: {e}\")"
      ],
      "metadata": {
        "id": "8V1taWs5gp5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercício 1\n",
        "\n",
        "Escreva um programa que acessa a página https://www.est.unb.br/ e lista o nome todos os professores aposentados.\n"
      ],
      "metadata": {
        "id": "8nHFviJDrOwQ"
      }
    }
  ]
}